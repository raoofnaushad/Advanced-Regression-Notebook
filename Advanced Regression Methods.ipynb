{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Description\n## Employee Attrition Rate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Write Description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.gridspec as gridspec\nfrom datetime import datetime\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport matplotlib.style as style\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport missingno as msno\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport scipy\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration Files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = '../input/employee-atrition-rate/Dataset'\ntrain = pd.read_csv(os.path.join(base_path, 'Train.csv'))\ntest = pd.read_csv(os.path.join(base_path, 'Test.csv'))\ntest_for_all = test\nprint(f\"Total number of train {len(train)} and test is {len(test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Utility Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\ndef submit(model, filename, npy = False):\n## Submission\n    submission = pd.read_csv(os.path.join(base_path, 'Test.csv'))\n    if npy == False:\n        preds = np.expm1(model.predict(X_sub))\n    else:\n        preds = np.expm1(model.predict(np.array(X_sub)))\n    empId = submission['Employee_ID'].tolist()\n    dict = {\"Employee_ID\": empId, \"Attrition_rate\": preds}\n    sub = pd.DataFrame(dict)\n    sub.to_csv(filename, index=False)\n\ndef overfit_reducer(df):\n    \"\"\"\n    This function takes in a dataframe and returns a list of features that are overfitted.\n    \"\"\"\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros / len(df) * 100 > 99.94:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\ndef plotting_3_chart(df, feature):\n    \"\"\"Plotting Target Variable\"\"\"\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n\ndef customized_scatterplot(y, x, c):\n    \"\"\"Plotting Scatter for Correlation\"\"\"\n        ## Sizing the plot. \n    style.use('fivethirtyeight')\n    plt.subplots(figsize = (12,8))\n    plt.title(c)\n    ## Plotting target variable with predictor variable(OverallQual)\n    sns.scatterplot(y = y, x = x);\n\ndef fixing_skewness(df):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n    ## Import necessary modules \n    from scipy.stats import skew\n    from scipy.special import boxcox1p\n    from scipy.stats import boxcox_normmax\n    \n    ## Getting all the data that are not of \"object\" type. \n    numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n    skewed_features = high_skew.index\n\n    for feat in skewed_features:\n        df[feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (train.dtypes == 'object')\nobject_cols = list(s[s].index)\nn = (train.dtypes != 'object')\nnum_cols = list(n[n].index)\nprint(f\"Number of object cols {len(object_cols)} and number of numerical cols {len(num_cols)}\")\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(train)  ## Missing Values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"train: \", missing_percentage(train), \"test: \",missing_percentage(test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations\n* There are multiple type of features\n* There are missing values\n* There are values whose variable identity is not relieved\n* The target variable is inbetween 0 and 1\n* There are 7 numerical and 17 object cols. Most of the numerical cols are class type cols","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Target Variable Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_3_chart(train, 'Attrition_rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n* Our target is not at all normally distributed.\n* Target data is right skewed.\n* Continue outliers and data is mainly observed between 0.1 -> 0.22**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Skewness  is {train['Attrition_rate'].skew()}\")\nprint(f\"Kurtosis  is {train['Attrition_rate'].kurt()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\n* Our target is right skewed or positive skewed with 2.056875960544357\n    This means that the mode is less than mean and median. Which means more employee have artition rate low than avg.\n### Kurtosis is the measure of outliers present in the distribution\n* Out data is Leptokurtic since it has value greater than 3 and that means that data are heavy-tailed or profusion of outliers.\nCheck out for [kurtosis](http://https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Checking for correlation among the target variable among the rest of the features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(train.corr()**2)['Attrition_rate'].sort_values(ascending = False)[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n* Found out that there is not much correlation among the target variable\n\n##### Let's check for each highly correlated among them with a scatter plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['VAR2', 'Work_Life_balance', 'Time_of_service', 'Post_Level', 'Age', 'VAR7', 'Pay_Scale', 'growth_rate', 'Time_since_promotion', \\\n       'VAR4', 'Travel_Rate']\nfor c in cols:\n    customized_scatterplot(train.Attrition_rate, train[c], c)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Some observations\nThe graphs doesn't give us much idea, it doesn't give us a linear relationship.\n* VAR2 > 1.5 has no data above 0.95\n* Work Life balance with 5 as value has very few data over 0.5 attrition rate.\n* When time of service is greater than 40 no attrition rate above 0.8   --------------> Can be used to tweek output\n* Payscale is 10 then few over 0.8 attrition\n\n### Mchine learning Perspective\n* I was not able to find any linear relationship here that can be utilised.\n* In categorical values also data is almost uniquely distributed. So not much use of it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Deleting outliers\nThere is not much outliers found in this case. So we will consider this later when one round completes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## save a copy of this dataset so that any changes later on can be compared side by side.\n# previous_train = train.copy()\n# ## Deleting those two values with outliers. \n# train = train[(train.Work_Life_balance == 5) & (train.Attrition_rate >= 0.8)]\n# train = train[(train.Time_of_service >= 40) & (train.Attrition_rate >= 0.6)]\n# train = train[(train.Pay_Scale == 1) & (train.Pay_Scale == 10) & (train.Attrition_rate >= 0.8)]\n# train = train[(train.Time_since_promotion==0) & (train.Attrition_rate >= 0.9)]\n# train.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the assumptions of Multiple Linear Regression\n\nLet's Check linearity with the most correlated functions\n> **VAR2, Work_Life_balance, Time_of_service, Post_Level, Age**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Plot sizing. \n# fig, (ax1, ax2) = plt.subplots(figsize = (12,8), ncols=2,sharey=False)\n# ## Scatter plotting for SalePrice and GrLivArea. \n# sns.scatterplot( x = train.VAR2, y = train.Attrition_rate,  ax=ax1)\n# ## Putting a regression line. \n# sns.regplot(x=train.VAR2, y=train.Attrition_rate, ax=ax1)\n\n# ## Scatter plotting for SalePrice and MasVnrArea. \n# sns.scatterplot(x = train.Work_Life_balance,y = train.Attrition_rate, ax=ax2)\n# ## regression line for MasVnrArea and SalePrice. \n# sns.regplot(x=train.Work_Life_balance, y=train.Attrition_rate, ax=ax2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.subplots(figsize = (12,8))\n# sns.residplot(train.VAR2, train.Attrition_rate);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Plot sizing. \n# fig, (ax1, ax2) = plt.subplots(figsize = (12,8), ncols=2,sharey=False)\n# ## Scatter plotting for SalePrice and GrLivArea. \n# sns.scatterplot( x = train.Age, y = train.Attrition_rate,  ax=ax1)\n# ## Putting a regression line. \n# sns.regplot(x=train.Age, y=train.Attrition_rate, ax=ax1)\n\n# ## Scatter plotting for SalePrice and MasVnrArea. \n# sns.scatterplot(x = train.Time_of_service,y = train.Attrition_rate, ax=ax2)\n# ## regression line for MasVnrArea and SalePrice. \n# sns.regplot(x=train.Time_of_service, y=train.Attrition_rate, ax=ax2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.subplots(figsize = (12,8))\n# sns.residplot(train.Age, train.Attrition_rate);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nLinearity is very poor and causes a lot of error value dealing with this as a linear problem\nHomoscedasticity ( Constant Variance ), we can say our data and independent variables has constant variance.\n> One way to fix this Heteroscedasticity is by using a transformation method like log-transformation or box-cox transformation. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Multivariate Normality ( Normality of Errors): The linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_3_chart(train, 'Attrition_rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transforming data to be more normalized\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Attrition_rate\"] = np.log1p(train[\"Attrition_rate\"])\n\n## Plotting the newly transformed response variable\nplotting_3_chart(train, 'Attrition_rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* No or Little multicollinearity: **Multicollinearity** is when there is a strong correlation between independent variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot fig sizing. \nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), \n            cmap=sns.diverging_palette(20, 220, n=200), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multicollinearity is there in very few features like Age, Time of service etc.\n\n### Inference\nIf I were using only multiple linear regression, I would be deleting these features from the dataset to fit better multiple linear regression algorithms. However, we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible outcome. Therefore, we will keep all the features for now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(columns=['Employee_ID'],axis=1, inplace=True)\ntest.drop(columns=['Employee_ID'],axis=1, inplace=True)\n\n## Saving the target values in \"y_train\". \ny = train['Attrition_rate'].reset_index(drop=True)\n\n# getting a copy of train\nprevious_train = train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Do some feature Engineering - I don't see much data for feature engineering. Let's use it later if needed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat((train, test)).reset_index(drop = True)\n## Dropping the target variable. \nall_data.drop(['Attrition_rate'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['Time_of_service'] = all_data.groupby('Post_Level')['Time_of_service'].transform( lambda x: x.fillna(x.mean()))\nall_data['Age'] = all_data.groupby('Time_of_service')['Age'].transform( lambda x: x.fillna(x.mean()))\nall_data['Pay_Scale'] = all_data.groupby('Post_Level')['Pay_Scale'].transform( lambda x: x.fillna(x.mean()))\nfor i in ['VAR4', 'VAR2', 'Work_Life_balance']:\n    all_data[i] = all_data[i].fillna(all_data[i].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### If you want you can convert some numerical to string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(all_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fixing Skewness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewed_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(all_data['Age'], color='red');\nsns.distplot(all_data['Time_of_service'], color='blue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fixing_skewness(all_data)\nsns.distplot(all_data['Age'], color='red');\nsns.distplot(all_data['Time_of_service'], color='blue');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create new features and Dropping features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['experience'] = all_data['Time_of_service'].apply(lambda x: 1 if x < 5 else (2 if x<20 else (3)))\nall_data = all_data.drop(['VAR3', 'VAR6'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Changing objective to categorical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_data.shape)\nfinal_features = pd.get_dummies(all_data).reset_index(drop=True)\nfinal_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_features.iloc[:len(y), :]\n\nX_sub = final_features.iloc[len(y):, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing Overfitted Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"overfitted_features = overfit_reducer(X)\noverfitted_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(overfitted_features, axis=1)\nX_sub = X_sub.drop(overfitted_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape,y.shape, X_sub.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Simple Approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train test s\nfrom sklearn.model_selection import train_test_split\n## Train test split follows this distinguished code pattern and helps creating train and test set to build machine learning. \nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .25, random_state = 0)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n## Call in the LinearRegression object\nlin_reg = LinearRegression(normalize=True, n_jobs=-1)\n## fit train and test data. \nlin_reg.fit(X_train, y_train)\n## Predict test data. \ny_pred = lin_reg.predict(X_test)\nprint ('RMSE with Linear Regression is %.6f'%mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit(lin_reg, 'linear_model.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Cross Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nlin_reg = LinearRegression()\ncv = KFold(shuffle=True, random_state=2, n_splits=10)\nscores = cross_val_score(lin_reg, X,y,cv = cv, scoring = 'neg_mean_absolute_error')\nprint ('%.8f'%scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regularization Models\n\n### Ridge ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Importing Ridge. \n# from sklearn.linear_model import Ridge\n# from sklearn.metrics import mean_absolute_error, mean_squared_error\n# ## Assiging different sets of alpha values to explore which can be the best fit for the model. \n# alpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]\n# temp_rss = {}\n# temp_mse = {}\n# loss_min = np.Inf\n# for i in alpha_ridge:\n#     ## Assigin each model. \n#     ridge = Ridge(alpha= i, normalize=True)\n#     ## fit the model. \n#     ridge.fit(X_train, y_train)\n#     ## Predicting the target value based on \"Test_x\"\n#     y_pred = ridge.predict(X_test)\n\n#     mse = mean_squared_error(y_test, y_pred)\n#     rss = sum((y_pred-y_test)**2)\n#     temp_mse[i] = mse\n#     temp_rss[i] = rss\n#     if mse<loss_min:\n#         loss_min = mse\n#         best_model = ridge\n#         best_i = i\n# print(loss_min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Importing Ridge. \n# from sklearn.linear_model import Lasso \n# from sklearn.metrics import mean_absolute_error, mean_squared_error\n# ## Assiging different sets of alpha values to explore which can be the best fit for the model. \n# alpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]\n# temp_rss = {}\n# temp_mse = {}\n# loss_min = np.Inf\n# for i in alpha_ridge:\n#     ## Assigin each model. \n#     lasso = Lasso(alpha= i, normalize=True)\n#     ## fit the model. \n#     lasso.fit(X_train, y_train)\n#     ## Predicting the target value based on \"Test_x\"\n#     y_pred = lasso.predict(X_test)\n\n#     mse = mean_squared_error(y_test, y_pred)\n#     rss = sum((y_pred-y_test)**2)\n#     temp_mse[i] = mse\n#     temp_rss[i] = rss\n#     if mse<loss_min:\n#         loss_min = mse\n#         best_model = ridge\n#         best_i = i\n# print(loss_min, i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting Model (Advanced Approach)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                                              alphas=alphas2, \n                                              random_state=42, \n                                              cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lightgbm = LGBMRegressor(objective='regression', \n#                                        num_leaves=4,\n#                                        learning_rate=0.01, \n#                                        n_estimators=5000,\n#                                        max_bin=200, \n#                                        bagging_fraction=0.75,\n#                                        bagging_freq=5, \n#                                        bagging_seed=7,\n#                                        feature_fraction=0.2,\n#                                        feature_fraction_seed=7,\n#                                        verbose=-1,\n#                                        )\nlightgbm = LGBMRegressor(n_estimators=1,  num_leaves=100, n_jobs=-1, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(ridge)\nprint(\"Ridge: {:.6f} ({:.6f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.6f} ({:.6f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.6f} ({:.6f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.6f} ({:.6f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.6f} ({:.6f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\n# score = cv_rmse(gbr)\n# print(\"gbr: {:.6f} ({:.6f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.6f} ({:.6f})\\n\".format(score.mean(), score.std()), datetime.now(), )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('START Fit and creating each submissions')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\nsubmit(stack_gen_model, 'stack_model.csv', npy=True)\n\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nsubmit(elastic_model_full_data, 'elastic_model_full_data.csv')\n\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\nsubmit(lasso_model_full_data, 'lasso_model_full_data.csv')\n\n\nprint('Ridge') \nridge_model_full_data = ridge.fit(X, y)\nsubmit(ridge_model_full_data, 'ridge_model_full_data.csv')\n\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\nsubmit(svr_model_full_data, 'svr_model_full_data.csv')\n\n\n# print('GradientBoosting')\n# gbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nsubmit(xgb_model_full_data, 'xgb_model_full_data.csv')\n\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nsubmit(lgb_model_full_data, 'lgb_model_full_data.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.2 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n#             (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Submission\nsubmission = pd.read_csv(os.path.join(base_path, 'Test.csv'))\npreds = np.expm1(blend_models_predict(X_sub))\nempId = submission['Employee_ID'].tolist()\ndict = {\"Employee_ID\": empId, \"Attrition_rate\": preds}\nsub = pd.DataFrame(dict)\nsub.to_csv('submission_blended.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}